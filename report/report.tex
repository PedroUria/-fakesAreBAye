\documentclass[man, floatsintext, 10pt]{apa6}
% floatsintext so that figures show up where we want them to (https://www.tug.org/pracjourn/2012-1/beitzel/beitzel.pdf)
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{harmony}  % Music stuff % https://martin-thoma.com/how-to-write-music-with-latex/
\usepackage{dirtytalk} % Quote


\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage[american]{babel}
\usepackage{ulem}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}

\title{fakesAreBAye}

\shorttitle{fakesAreBAye}

\author{Sean Pili and Pedro Uria \\ Bayesian Methods for Data Science}

\affiliation{GWU}

\begin{document}
\maketitle

\section{Introduction}

In this project we develop models via Bayesian and Machine Learning methods in order to tell whether a restaurant review is real or fake, as well as to analyze various predictors that play or do not play a role in the matter. This problem is of great interest to the consumer and the food service industry community because a great majority of people read restaurant reviews online in order to make a well-informed decision on their meal destination. These reviews greatly affect consumers' prior beliefs about a particular business, and, unfortunately, it has been known for a while that some of these businesses pay individuals to write fake reviews, either positive towards their own products and services, or negatives towards their competitors. This immoral activity hurts consumers who may trust these fake reviews, even when they may be wary of them, because at first glance and even under careful inspection, fake reviews can very well look like real reviews. In order to take this into account, we use a combination of text but also behavioral features based on the reviewers previous activities.  While we focus on restaurants, this work could very well be extended to other kinds of reviews, such as hotels, services, and products in general.

A good amount of work has previously been done in this matter. Mukherjee et. al used, a combination of  behavioral features, and n-gram features to predict whether Yelp reviews for restaurants and hotels were real or fake.  They achieved accuracy of .828, recall of .879 and precision of .821 using behavioral features alone. When adding n-grams, they increased their accuracy to .841, their precision to .834, but their recall fell to .871 Barbado et. al  extended their work, using their own behavioral features and linguistic features, as well as well as metadata about the reviewer accounts from an e-commerce website to predict whether the e-commerce reviews were fake and achieved an F1 score of .82. Maajar et. all used behavorial data and metadata about reviewers only to achieve a recall of .91 when using them to classify android and ios apps as real or fake. 

Obviously, the only way to train a classifier to detect fake reviews is to know whether each review the model is trained on is real or fake. Of the review platforms we  searched in our literature review, only Yelp makes fake reviews public. That is to say, Yelp publishes whether its internal algorithm detected whether a given review is real or fake. Scraping the reviews was a tedious task, so we asked Mukherjee et. al to send us the Yelp data they used for their analysis and they provided it to us. 

\section{Data}

\paragraph{Raw data} The dataset we used was collected by Mukherjee et. al in order to analyze Yelp's review filter. It consists of  $61,541$ reviews of restaurants located in Chicago that were scraped from Yelp's website, and includes the review itself, together with the date, a unique review id, reviewer id, product id, star rating and label. 53400  of the reviews were real and  8141 were fake, leaving us with an imbalanced dataset with the minority class making up 13 percent of the data. The star rating ranges from 1 to 5, and only has integer values (cannot have 2.3 stars.)  Each review was automatically labeled as either real or fake by Yelp's filter algorithm. While this ground truth may not be completely accurate, Yelp is bold enough to make the fake reviews public, meaning that they have a great amount of trust on their model, which has also been under study and deemed to be fairly precise by Mukherjee et. al. It is worth noting that Yelp has access to additional private user data, which is most likely being used to improve their system. The goal of our project is thus to provide other companies with a baseline of how to implement a fake review filter algorithm, as well as analyze the public features that are most relevant to this problem. 

Upon further inspection of the data, we noticed that many of the users had only one review, which made calculating the maximum cosine similarity among user reviews for one user impossible, and make all of the features that incorporate averages less meaningful. Therefore, we dropped all users from the dataset that only had one review. After doing this, the dataset contained 37936 reviews, 35850 of which were real, and 2086 were fake, meaning we had a very imbalanced dataset with the minority class, fake reviews, making up only 5.8 percent of our data. 

\vspace{2mm}

\paragraph{Behavioral Features} Mukherjee et. al created 5 behavioral features, four of which we used in our analysis. The first feature we used was MNR, or the Maximum number of Reviews a user has posted in a day. The authors state that 75 percent of spammers wrote 6+ reviews a day (i.e. the first quartile of the MNR variable was 5)  when looking across both their restaurant and hotel datasets, but in our restaurant dataset, the first quartile for MNR is 1. Furthermore, the 90th percentile of MNR is 2, meaning that 90 percent of the reviewers writing fake reviews wrote at most 2 or fewer reviews per day. it could be possible that the restaurant dataset is much smaller than the hotels dataset and the spammers in the hotel dataset wrote many more fake reviews per day.  Oddly, the 75th percentile of MNR is actually higher for non-spammers (2), than spammers (1), meaning that a higher percentage of  Although the difference may be negligible, it is strange that the trend the paper discusses is reversed in the restaurant dataset. The second behavioral feature, PR, is the percentage of user ratings that are 4 stars or higher. Mukherjee et. al reported that 85 percent of spammers rated more than 80 percent of their reviews positive, and that roughly 50 percent of regular reviewers rated reviews as positive across both restaurants and hotels combined.  However in our restaurant dataset, the median of the PR variable is .5 for both spammers and non-spammers. The third behavioral feature is average review length, which is much longer for real reviewers than fake reviewers in the literature when considering both restaurants and hotels.  The fourth behavioral feature is reviewer deviation, which is calculated as follows: the average star rating is calculated for each productID (aka restaurant ID) in the dataset, the absolute deviation from that average rating is calculated for every reviewer that reviewed that product. That process is repeated for every product in the dataset. After that, an average is taken across those deviations for each reviewer in the dataset. So, if a spammer gave three restaurants that have an average rating of three stars ratings of 5, 5 and 4 stars, their reviewer deviation statistic would be 4.67. This variable's summary statistics differ with those described in our literature review. Although in both cases, the reviewer deviation for spammers is higher than that non-spammers, it is not higher to the extreme  mentioned in the literature review. I.e. 80 percent of the real reviewers had a reviewer deviation of greater than .45 as opposed to .69 for spammers in our dataset, but in the literature review, 80 percent of the fake reviewers had a reviewer deviation of greater than 2.5 and and 80 percent of real reviewers had a reviewer deviation greater than .6. The final behavioral feature is maximum cosine similarity (mcs) between reviews of a given reviewer. They did not specify if any pre-processing such as tokenization, lemmatization or stemming was applied to the text before cosine similarity was calculated, so we do not expect the summary statistics of that variable to match Mukherjee et. al's exactly. For reference, we tokenized the text with nltk and applied the porter stemmer to the tokens. However, we again find it odd that the average cosine similarity of spammers is lower than the cosine similarity  of the real reviewers, because Mukherjee et. al reported the opposite results.
 \vspace{2mm}
 
\paragraph{Text Features} For computing the text features, we decided to use a state-of-the-art Neural Network called BERT, which will be briefly explained in the next section. We do not delve into this part very deeply, as we are aware that not even a human can distinguish well-crafted fake reviews from genuine reviews based on the text alone.


\section{Modeling}

In this section we discuss the models used to tackle this problem. This is a binary classification task, in which we want to be able to correctly label a restaurant online review as either real or fake. We have discussed previous work on the matter, and incorporated some of the behavioral features used into our own models. However, we have taken a new approach for the text features.

\vspace{2mm}

\paragraph{BERT} In our case, we used BERT as a language feature extractor. BERT is a Neural Network that has achieved state-of-the-art results in many NLP tasks, including classification. Although describing this language model is not of interest here, and thus will be treated as a black box, it is noteworthy that the authors have a lot of experience using BERT-like models. The input to these models can be thought of as the raw text for the purpose of this work, and in order to extract the features for each review, we add a linear layer on top of BERT, that maps its 768 dimensional output to a 3-dimensional dense vector, and another linear layer with a sigmoid output function that maps this vector to the probability of a review being real or fake. Therefore, once BERT is trained by minimizing a Binary Cross-Entropy performance index on our training data to classify the reviews, we go forward on our test data and use the intermediate 3-dimensional vector output as our text features. 

Regarding the specifics of training BERT, we decided to cut the reviews at 100 tokens long, as BERT's time complexity is quadratic to the number of tokens in our reviews, and if the text actually reveals whether a review is real or fake, we speculated such pattern would also be present in the first one hundred words. We only used the first 4 layers of this model, a batch size of 32 reviews, and the Adam weight-decay optimizer with a learning rate of $e^{-5}$ and an epsilon of $e^{-8}$. In order to force the features to be more meaningful towards fake reviews, we weighted our loss during training using the proportion of real vs fake reviews. All the training was done in PyTorch and we used the huggingface transformers implementation of BERT, starting from its base uncased pre-trained version.

\vspace{2mm}

\paragraph{Bayesian Logistic Regression} Once all the features were ready, we proceeded to train logistic regression models under the Bayesian probabilistic approach using R and JAGS. In order to do so, we need to set some prior distributions to each of the model's parameters before we run the MCMC sampling process that allows us to infer their posteriors. Such priors will be discussed in the next section. In regards to the specific bayesian model, we use the following hierarchical approach 

\[\beta_i \sim \text{ prior distribution} \] \[ \uparrow \] \[ \mu = \frac{1}{1 + e^{-\big(\beta_0 + \sum_i \beta_i x_i \big)}} \] \vspace{0.01mm}  \[ \uparrow \] \[ y \sim\ \text{bernoulli} (\mu) \]

in which the target variable follows a Bernoulli distribution, with its probability following the logistic equation. Each of the logistic coefficients also follow the prior distributions mentioned above. By the iterative Markov-Chain Monte-Carlo algorithm, we sample from the posterior distributions of each of our coefficients until the chains converge. For a more detailed explanation of the theory behind this learning process, you can read \textit{Doing Bayesian Analysis} by John K. Kruschk. We also experiment with more advanced methods such as Robust Logistic Regression, which helps to deal with outliers by incorporating into the model a small chance ($\alpha/2$) of a data point being randomly generated, and variable selection, in order to deal, for the most part, with the BERT features. Variable selection is achieved by incorporating a multiplier to each logistic coefficient that can only be 0 or 1. Thus, if this multiplier ($\delta_i$) is 0, the feature $i$ will not be used in the model. Of course, these new parameters also need priors, which come from Bernoulli distributions with different probabilities. The most general model we used is therefore described as follows:

 \[\beta_i \sim \text{ prior distribution}, \hspace{2mm} \delta_i \sim \text{ Bernoulli prior distribution}, \hspace{2mm}\  \alpha \sim \text{ beta prior distribution (1, 9)} \] \[ \uparrow \] \[ \mu = \frac{1}{2} \alpha + (1 - \alpha) \frac{1}{1 + e^{-\big(\beta_0 + \sum_i \delta_i \beta_i x_i \big)}} \] \vspace{0.01mm}  \[ \uparrow \] \[ y \sim\ \text{bernoulli} (\mu) \]
 
In the next section we elaborate on different priors and their posteriors, the use of the advanced approaches, and our inference results.

\section{Experiments and Results}

\paragraph{Overview} Although most of the experimental work focused on MCMC, we also experimented when training BERT, until we got a decent model by using the approach mentioned at the beginning of the previous section. Such model ..... TODO: results. The experiments will not be discussed as they are not relevant for this work. We decided to keep the number of BERT features very small compared to the original size (3 vs 768), in order to avoid overwhelming the likely more important behavioral features . While we standardized the rest of the features, we did not do so for BERT's, because they were already within a [-3, 3] range TODO: change to the filtered data, and doing so would mess with the our approach for choosing the priors (explained in the next paragraph). We conducted several experiments in order to arrive at chains with good convergence and the desired properties. In this section we explain the most relevant, as well as present and analyze our results. 

\vspace{2mm}

\paragraph{Priors} TODO: Mention priors for behavioral features. Regarding the BERT features, we decided to go with normal distributions, using the BERT weights as means. That is, on our final classification layer, we basically have the following equation: $\text{prob}_{\text{fake}} = \text{sigmoid} (w_0 p_0 + w_1 p_1 + w_2 p_2 + b)$, where $p_i$ is our feature $i$ and $w_i$ is its weight. Given the fact that the sum inside the sigmoid is exactly a part of the sum instead our bayesian model, using the $w_i$ as the mean for the $p_i$ priors is likely the best we can do, even more so when realizing that we do not know what the BERT features really mean. We also use the bias as the mean for our intercept's prior normal distribution, for the very same reason. The bias and weights were ($-0.5025951$, $-0.37428102$ and $0.35757005$) TODO: change to the filtered data


\vspace{2mm}

\paragraph{Prediction Methods} We experimented with a variety of methods to use our Bayesian GLM's to make predictions on our test set. The first method was to find the mode of each beta coefficient from the MCMC-generated samples (after rounding them to 2 decimal places because there could be infinitely many continuous beta values), then substitute those and the test data into equation. For the second method, we tried to take the output of the logistic equation before using it as the probability of success to generate a Bernoulli variable and use thresholding (between .1 and .9) to determine if the output would be real or fake. The latter approach typically generated higher accuracy than the former approach with thresholds between .4-.6 because values generated from a Bernoulli distribution will be 1 or 0 as long as the probability of success is not exactly 0 or 1. I.e., even though the outputs of the logistic equation of our Betas and the data may output values close to 1 or 0, such as .9 or .1, which are synonymous to outputting very confident predictions that reviews are real or fake, using those values as the probability of success in a Bernoulli trial to generate our final prediction will occasionally yield counter-intuitive final predictions. So, for example, if the output of the logistic equation using the mode of the betas from the MCMC samples generated from our data on one row of the test set was .8, we would predict the review in that row was fake 20 times out of 100 if we used a bernoulli distribution with the output of the logistic function as the probability of success, which is alarming, as it could greatly reduce the accuracy of our predictions. Although the thresholding approach yielded higher accuracy than using bernoulli trials, the recall from both methods was similar using a threshold of .5. However, we could increase recall by lowering the threshold. The final method we used to generate predictions was as follows: first, we would apply logistic equation with each set of N Betas generated from the mcmc and apply thresholding (with values between .1 and .9) to generate N predictions for each row in the test set and then use majority voting to yield the final prediction for each row in the test set. This method was typically less accurate than using only the mode of the betas to make predictions and applying thresholding to the output of the logistic equation, but it generated much higher recall using an accuracy of .5

\section{Conclusions} 

TODO https://arxiv.org/pdf/1903.12452.pdf

https://link.springer.com/article/10.1007/s10664-019-09706-9
\end{document}

