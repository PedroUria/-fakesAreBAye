\documentclass[man, floatsintext, 10pt]{apa6}
% floatsintext so that figures show up where we want them to (https://www.tug.org/pracjourn/2012-1/beitzel/beitzel.pdf)
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{harmony}  % Music stuff % https://martin-thoma.com/how-to-write-music-with-latex/
\usepackage{dirtytalk} % Quote


\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage[american]{babel}
\usepackage{ulem}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}

\title{fakesAreBAye}

\shorttitle{fakesAreBAye}

\author{Sean Pili and Pedro Uria \\ Bayesian Methods for Data Science}

\affiliation{GWU}

\begin{document}
\maketitle

\section{Introduction}

In this project we develop models via Bayesian and Machine Learning methods in order to tell whether a restaurant review is real or fake, as well as to analyze various predictors that play or do not play a role in the matter TODO: Change once our results are clear. This problem is of great interest to the consumer and the food service industry community because a great majority of people read restaurant reviews online in order to make a well-informed decision on their meal destination. These reviews greatly affect consumers' prior beliefs about a particular business, and, unfortunately, it has been known for a while that some of these businesses pay individuals to write fake reviews, either positive towards their own products and services, or negatives towards their competitors. This immoral activity hurts consumers who may trust these fake reviews, even when they may be wary of them, because at first glance and even under careful inspection, fake reviews can very well look like real reviews. In order to take this into account, we use a combination of text but also behavioral features based on the reviewers previous activities.  While we focus on restaurants, this work could very well be extended to other kinds of reviews, such as hotels, services, and products in general.

A good amount of work has previously been done in this matter. Mukherjee et. al used 

\section{Data}

\paragraph{Raw data} The dataset we used was collected by Mukherjee et. al in order to analyze Yelp's review filter TODO: Note. It consists of  $61,541$ reviews of restaurants located in Chicago that were scraped from Yelp's website, and includes the review itself, together with the date, a unique review id, reviewer id, product id, star rating and label. The star rating ranges from 1 to 5, and only has integer values (cannot have 2.3 stars.) Each review was automatically labeled as either real or fake by Yelp's filter algorithm. While this ground truth may not be completely accurate, Yelp is bold enough to make the fake reviews public, meaning that they have a great amount of trust on their model, which has also been under study and deemed to be fairly precise Mukherjee et. al. It is worth noting that Yelp has access to additional private user data, which is most likely being used to improve their system. The goal of this project is thus to provide other companies with a baseline of how to implement a fake review filter algorithm, as well as analyze the public features that are most relevant to this problem.

\vspace{2mm}

\paragraph{Behavioral Features} Mukherjee et. al created 5 behavorial features, four of which we used in our analysis. The first feature we used was MNR, or the Maximum number of Reviews a user has posted in a day. The authors states that 75 percent of spammers wrote 6+ reviews a day (i.e. the 25h percentile if 5), when looking across both their restaurant and hotel datasets, but in our restaurant dataset, the 25th percentile for MNR is 1, which is much differne than what the paper suggested, unless somehow the scammers who show the suspicious behaviors are only posting many revies per day on hotel websites and not the websites of restaurants and that is skewing the average. Oddly, the 75th percentile of MNR is actually higher for non-spammers,2, than spammers, 1. Although the difference may be negligible, it is strange that the trend the paper discusses is reversed in the restaurant dataset. The second behavorial feature, PR, is the percentage of user ratings that are 4 stars or higher. This feature is slightly higher for spammers than non-spammers, but not by much. The third behavorial feature is average review length, which is much longer for real reviewers than fake reviewers. The fourth behavorial feature is reviewer deviation, which is calculated as follows: the average star rating is calculated for each productID (aka restaurant ID) in the dataset, the absolute deviation from that average rating is calculated for every reviewer that reviewed that product. That process is repeated for every product in the dataset. After that, an average is taken across those deviations for each reviewer in the dataset. So, if a spammer gave three restaurants that have an average rating of three stars ratings of 5, 5 and 4 stars, their reviewer deviation statistic would be 4.67.

 \vspace{2mm}
 
\paragraph{Text Features} For computing the text features, we decided to use a state-of-the-art Neural Network called BERT, which will be briefly explained in the next section. We do not delve into this part very deeply, as we are aware that not even a human can distinguish well-crafted fake reviews from genuine reviews based on the text alone.


\section{Modeling}

In this section we discuss the models used to tackle this problem. This is a binary classification task, in which we want to be able to correctly label a restaurant online review as either real or fake. We have discussed previous work on the matter, and incorporated some of the behavioral features used into our own models. However, we have taken a new approach for the text features.

\vspace{2mm}

\paragraph{BERT} In our case, we used BERT as a language feature extractor. BERT is a Neural Network that has achieved state-of-the-art results in many NLP tasks, including classification. Although describing this language model is not of interest here, and thus will be treated as a black box, it is noteworthy that the authors have a lot of experience using BERT-like models. The input to these models can be thought of as the raw text for the purpose of this work, and in order to extract the features for each review, we add a linear layer on top of BERT, that maps its 768 dimensional output to a 3-dimensional dense vector, and another linear layer with a sigmoid output function that maps this vector to the probability of a review being real or fake. Therefore, once BERT is trained by minimizing a Binary Cross-Entropy performance index on our training data to classify the reviews, we go forward on our test data and use the intermediate 3-dimensional vector output as our text features. 

Regarding the specifics of training BERT, we decided to cut the reviews at 100 tokens long, as BERT's time complexity is quadratic to the number of tokens in our reviews, and if the text actually reveals whether a review is real or fake, we speculated such pattern would also be present in the first one hundred words. We only used the first 4 layers of this model, a batch size of 32 reviews, and the Adam weight-decay optimizer with a learning rate of $e^{-5}$ and an epsilon of $e^{-8}$. In order to force the features to be more meaningful towards fake reviews, we weighted our loss during training using the proportion of real vs fake reviews. All the training was done in PyTorch and we used the huggingface transformers implementation of BERT, starting from its base uncased pre-trained version.

\vspace{2mm}

\paragraph{Bayesian Logistic Regression} Once all the features were ready, we proceeded to train logistic regression models under the Bayesian probabilistic approach using R and JAGS. In order to do so, we need to set some prior distributions to each of the model's parameters before we run the MCMC sampling process that allows us to infer their posteriors. TODO... Regarding the BERT features, we decided to go with normal distributions, using the BERT weights as their mean. That is, on our final classification layer, we basically have the following equation: $\text{prob}_{\text{fake}} = \text{sigmoid} (w_0 p_0 + w_1 p_1 + w_2 p_2 + b)$, where $p_i$ is our feature $i$ and $w_i$ is its weight. Given the fact that the sum inside the sigmoid is exactly a part of the sum instead our bayesian model, using the $w_i$ as the mean for the $p_i$ priors is likely the best we can do, even more so when realizing that we do not know what the BERT features really mean. TODO: bias --> Intercept....!!!!

In regards to the specific bayesian model, we use the following hierarchical approach 

\[\beta_i \sim \text{ prior distribution} \] \[ \uparrow \] \[ \mu = \frac{1}{1 + e^{-\big(\beta_0 + \sum_i \beta_i x_i \big)}} \] \vspace{0.01mm}  \[ \uparrow \] \[ y \sim\ \text{bernoulli} (\mu) \]

in which the target variable follows a Bernoulli distribution, with its probability following the logistic equation. Each of the logistic coefficients also follow the prior distributions mentioned above. By the iterative Markov-Chain Monte-Carlo algorithm, we sample from the posterior distributions of each of our coefficients until the chains converge. For a more detailed explanation of the theory behind this learning process, you can read ... TODO.  We also experiment with more advanced methods such as Robust Logistic Regression, which helps to deal with outliers by incorporating into the model a small chance ($\alpha/2$) of a data point being randomly generated, and variable selection, in order to deal, for the most part, with the BERT features. Variable selection is achieved by incorporating a multiplier to each logistic coefficient that can only be 0 or 1. Thus, if this multiplier ($\delta_i$) is 0, the feature $i$ will not be used in the model. Of course, these new parameters also need priors, which come from Bernoulli distributions with different probabilities. The most general model we used is therefore described as follows:

 \[\beta_i \sim \text{ prior distribution}, \hspace{2mm} \delta_i \sim \text{ Bernoulli prior distribution}, \hspace{2mm}\  \alpha \sim \text{ beta prior distribution (1, 9)} \] \[ \uparrow \] \[ \mu = \frac{1}{2} \alpha + (1 - \alpha) \frac{1}{1 + e^{-\big(\beta_0 + \sum_i \delta_i \beta_i x_i \big)}} \] \vspace{0.01mm}  \[ \uparrow \] \[ y \sim\ \text{bernoulli} (\mu) \]
 
In the next section we elaborate on different priors and their posteriors, the use of the advanced approaches, and our inference results.

\section{Experiments and Results}

\paragraph{Overview} Although most of the experimental work focused on MCMC, we also experimented when training BERT, until we got a decent model by using the approach mentioned at the beginning of the previous section. Such model ..... TODO: results. The experiments will not be discussed as they are not relevant for this work. We decided to keep the number of BERT features very small compared to the original size (3 vs 768), in order to avoid overwhelming the likely more important behavioral features. The weights for these features ($-0.5025951$, $-0.37428102$ and $0.35757005$) were used as means of their prior normal distributions, with a standard deviation of TODO . While we standardized the rest of the features, we did not do so for BERT's, because they were already within a [-3, 3] range, and doing so would mess with the our approach for choosing the priors. We conducted several experiments in order to arrive at chains with good convergence and the desired properties. In this section we explain the most relevant, as well as present and analyze our results. 

\vspace{2mm}

\paragraph{Prediction Methods} We experimented with a variety of methods to use our Bayesian GLM's to make predictions on our test set. The first method was to find the mode of each beta coefficient from the mcmc samples generated from training (after rounding them to 2 decimal places because there could be infinitely many continous beta values), then substitute those and the test data into the forumla we derived in the 'Bayesian Logistic Regression' section above. The second method we tried was to take the output of the logistic equation before using it as the probability of success to generate a bernoulli variable and use thresholding (between .1 and .9) to determine if the output would be real or fake. The latter approach typically generated higher accuracy than the former approach with thresholds between .4-.6 because values generated from a bernoulli distribution will be 1 or 0 as long as the probability of success is not exactly 0 or 1. I.e., even though the outputs of the logistic equation of our Betas and the data may output values close to 1 or 0, such as .9 or .1, which are synonomous to outputting very confident predictions that reviews are real or fake, using those values as the probability of success in a bernoulli trial to generate our final prediction will occasionally yield counter-intuitive final predictions. So, for example, if the output of the logistic equation using the mode of the betas from the mcmc samples generated from our data on one row of the test set was .8, we would predict the review in that row was fake 20 times out of 100 if we used a bernoulli distribution with the output of the logistic function as the probability of success, which is alarming, as it could greatly reduce the accuracy of our predictions. Although the thresholding approach yielded higher accuracy than using bernoulli trials, the recall from both methods was similar using a threshold of .5. However, we could increase recall by lowering the threshold. The final method we used to generate predictions was as follows: first, we would apply logistic equation with each set of N Betas generated from the mcmc and apply thresholding (with values between .1 and .9) to generate N predictions for each row in the test set and then use majority voting to yield the final prediction for each row in the test set. This method was typically less accurate than using only the mode of the betas to make predictions and applying thresholding to the output of the logistic equation, but it generated much higher recall using an accuracy of .5

\section{Conclusions} 

TODO

\end{document}

